:mod:`gpflux.layers`
====================

.. py:module:: gpflux.layers

.. autoapi-nested-parse::

   Layers



Subpackages
-----------
.. toctree::
   :titlesonly:
   :maxdepth: 3

   basis_functions/index.rst


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   bayesian_dense_layer/index.rst
   gp_layer/index.rst
   latent_variable_layer/index.rst
   likelihood_layer/index.rst
   trackable_layer/index.rst


Package Contents
----------------

.. class:: BayesianDenseLayer(input_dim: int, output_dim: int, num_data: int, w_mu: Optional[np.ndarray] = None, w_sqrt: Optional[np.ndarray] = None, activation: Optional[Callable] = None, is_mean_field: bool = True, temperature: float = 0.0001, returns_samples: bool = True)


   Bases: :class:`gpflux.layers.trackable_layer.TrackableLayer`

   A Bayesian dense layer for variational Bayesian neural networks

   A Bayesian dense layer for variational Bayesian neural nets. This layer holds the
   weight mean and sqrt as well as the temperature for cooling (or heating) the posterior.

   :param input_dim: The layer's input dimension (excluding bias)
   :param output_dim: The layer's output dimension
   :param num_data: number of data points
   :param w_mu: Initial value of the variational mean (weights + bias)
   :param w_sqrt: Initial value of the variational Cholesky (covering weights + bias)
   :param activation: The type of activation function (None is linear)
   :param is_mean_field: Determines mean field approximation of the weight posterior
   :param temperature: For cooling or heating the posterior
   :param returns_samples: If True, return samples on calling the layer,
        Else return mean and variance

   .. method:: build(self, input_shape: gpflux.types.ShapeType) -> None

      Build the variables necessary on first call


   .. method:: predict_samples(self, inputs: gpflow.base.TensorType, *, num_samples: Optional[int] = None, full_output_cov: bool = False, full_cov: bool = False, whiten: bool = False) -> tensorflow.Tensor

      Make a sample predictions at N test inputs, with input_dim = D, output_dim = Q. Return a
      sample, and the conditional mean and covariance at these points.

      :param inputs: the inputs to predict at. shape [N, D]
      :param num_samples: the number of samples S, to draw.
          shape [S, N, Q] if S is not None else [N, Q].
      :param full_output_cov: assert to False since not supported for now
      :param full_cov: assert to False since not supported for now
      :param whiten: assert to False since not sensible in Bayesian neural nets


   .. method:: call(self, inputs: gpflow.base.TensorType, training: Optional[bool] = False) -> Union[(tf.Tensor, MeanAndVariance)]

      The default behaviour upon calling the BayesianDenseLayer()(X)


   .. method:: prior_kl(self) -> tensorflow.Tensor

      The KL divergence from the variational distribution to the prior
      :return: KL divergence from N(w_mu, w_sqrt) to N(0, I)



.. class:: GPLayer(kernel: gpflow.kernels.MultioutputKernel, inducing_variable: gpflow.inducing_variables.MultioutputInducingVariables, num_data: int, mean_function: Optional[MeanFunction] = None, *, num_samples: Optional[int] = None, full_cov: bool = False, full_output_cov: bool = False, num_latent_gps: int = None, whiten: bool = True, name: Optional[str] = None, verbose: bool = False)


   Bases: :class:`tensorflow_probability.layers.DistributionLambda`

   A sparse variational multioutput GP layer. This layer holds the kernel,
   inducing variables and variational distribution, and mean function.

   :param kernel: The multioutput kernel for this layer.
   :param inducing_variable: The inducing features for this layer.
   :param num_data: The number of points in the training dataset (see :attr:`num_data`).
   :param mean_function: The mean function that will be applied to the
       inputs. Default: :class:`~gpflow.mean_functions.Identity`.

       .. note:: The Identity mean function requires the input and output
           dimensionality of this layer to be the same. If you want to
           change the dimensionality in a layer, you may want to provide a
           :class:`~gpflow.mean_functions.Linear` mean function instead.

   :param num_samples: The number of samples to draw when converting the
       :class:`~tfp.layers.DistributionLambda` into a `tf.Tensor`, see
       :meth:`_convert_to_tensor_fn`. Will be stored in the
       :attr:`num_samples` attribute.  If `None` (the default), draw a
       single sample without prefixing the sample shape (see
       :class:`tfp.distributions.Distribution`'s `sample()
       <https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution#sample>`_
       method).
   :param full_cov: Sets default behaviour of calling this layer
       (:attr:`full_cov` attribute):
       If `False` (the default), only predict marginals (diagonal
       of covariance) with respect to inputs.
       If `True`, predict full covariance over inputs.
   :param full_output_cov: Sets default behaviour of calling this layer
       (:attr:`full_output_cov` attribute):
       If `False` (the default), only predict marginals (diagonal
       of covariance) with respect to outputs.
       If `True`, predict full covariance over outputs.
   :param num_latent_gps: The number of (latent) GPs in the layer
       (which can be different from the number of outputs, e.g. with a
       :class:`~gpflow.kernels.LinearCoregionalization` kernel).
       This is used to determine the size of the
       variational parameters :attr:`q_mu` and :attr:`q_sqrt`.
       If possible, it is inferred from the *kernel* and *inducing_variable*.
   :param whiten: If `True` (the default), uses the whitened parameterisation
       of the inducing variables; see :attr:`whiten`.
   :param name: The name of this layer.
   :param verbose: The verbosity mode. Set this parameter to `True`
       to show debug information.

   .. attribute:: num_data
      :annotation: :int

      The number of points in the training dataset. This information is used to
      obtain the correct scaling between the data-fit and the KL term in the
      evidence lower bound (ELBO).


   .. attribute:: whiten
      :annotation: :bool

      This parameter determines the parameterisation of the inducing variables.

      If `True`, this layer uses the whitened (or non-centred) representation, in
      which (at the example of inducing point inducing variables) ``u = f(Z) =
      cholesky(Kuu) v``, and we parameterise an approximate posterior on ``v`` as
      ``q(v) = N(q_mu, q_sqrt q_sqrtᵀ)``. The prior on ``v`` is ``p(v) = N(0, I)``.

      If `False`, this layer uses the non-whitened (or centred) representation,
      in which we directly parameterise ``q(u) = N(q_mu, q_sqrt q_sqrtᵀ)``. The
      prior on ``u`` is ``p(u) = N(0, Kuu)``.


   .. attribute:: num_samples
      :annotation: :Optional[int]

      The number of samples drawn when coercing the output distribution of
      this layer to a `tf.Tensor`. (See :meth:`_convert_to_tensor_fn`.)


   .. attribute:: full_cov
      :annotation: :bool

      This parameter determines the behaviour of calling this layer. If `False`, only
      predict or sample marginals (diagonal of covariance) with respect to inputs.
      If `True`, predict or sample with the full covariance over the inputs.


   .. attribute:: full_output_cov
      :annotation: :bool

      This parameter determines the behaviour of calling this layer. If `False`, only
      predict or sample marginals (diagonal of covariance) with respect to outputs.
      If `True`, predict or sample with the full covariance over the outputs.


   .. attribute:: q_mu
      :annotation: :Parameter

      The mean of ``q(v)`` or ``q(u)`` (depending on whether :attr:`whiten`\ ed
      parametrisation is used).


   .. attribute:: q_sqrt
      :annotation: :Parameter

      The lower-triangular Cholesky factor of the covariance of ``q(v)`` or ``q(u)``
      (depending on whether :attr:`whiten`\ ed parametrisation is used).


   .. method:: predict(self, inputs: gpflow.base.TensorType, *, full_cov: bool = False, full_output_cov: bool = False) -> Tuple[(tf.Tensor, tf.Tensor)]

      Make a prediction at N test inputs for the Q outputs of this layer,
      including the mean function contribution.

      The covariance and its shape is determined by *full_cov* and *full_output_cov* as follows:

      +--------------------+---------------------------+--------------------------+
      | (co)variance shape | ``full_output_cov=False`` | ``full_output_cov=True`` |
      +--------------------+---------------------------+--------------------------+
      | ``full_cov=False`` | [N, Q]                    | [N, Q, Q]                |
      +--------------------+---------------------------+--------------------------+
      | ``full_cov=True``  | [Q, N, N]                 | [N, Q, N, Q]             |
      +--------------------+---------------------------+--------------------------+

      :param inputs: The inputs to predict at, with a shape of [N, D], where D is
          the input dimensionality of this layer.
      :param full_cov: Whether to return full covariance (if `True`) or
          marginal variance (if `False`, the default) w.r.t. inputs.
      :param full_output_cov: Whether to return full covariance (if `True`)
          or marginal variance (if `False`, the default) w.r.t. outputs.

      :returns: posterior mean (shape [N, Q]) and (co)variance (shape as above) at test points


   .. method:: call(self, inputs: gpflow.base.TensorType, *args: List[Any], **kwargs: Dict[(str, Any)]) -> tensorflow.Tensor

      The default behaviour upon calling this layer.

      This method calls the `tfp.layers.DistributionLambda` super-class
      `call` method, which constructs a `tfp.distributions.Distribution`
      for the predictive distributions at the input points
      (see :meth:`_make_distribution_fn`).
      You can pass this distribution to `tf.convert_to_tensor`, which will return
      samples from the distribution (see :meth:`_convert_to_tensor_fn`).

      This method also adds a layer-specific loss function, given by the KL divergence between
      this layer and the GP prior (scaled to per-datapoint).


   .. method:: prior_kl(self) -> tensorflow.Tensor

      Returns the KL divergence ``KL[q(u)∥p(u)]`` from the prior ``p(u)`` to
      the variational distribution ``q(u)``.  If this layer uses the
      :attr:`whiten`\ ed representation, returns ``KL[q(v)∥p(v)]``.


   .. method:: _make_distribution_fn(self, previous_layer_outputs: gpflow.base.TensorType) -> tensorflow_probability.distributions.Distribution

      Construct the posterior distributions at the output points of the previous layer,
      depending on :attr:`full_cov` and :attr:`full_output_cov`.

      :param previous_layer_outputs: The output from the previous layer,
          which should be coercible to a `tf.Tensor`


   .. method:: _convert_to_tensor_fn(self, distribution: tensorflow_probability.distributions.Distribution) -> tensorflow.Tensor

      Convert the predictive distributions at the input points (see
      :meth:`_make_distribution_fn`) to a tensor of :attr:`num_samples`
      samples from that distribution.
      Whether the samples are correlated or marginal (uncorrelated) depends
      on :attr:`full_cov` and :attr:`full_output_cov`.


   .. method:: sample(self) -> gpflux.sampling.sample.Sample

      .. todo:: TODO: Document this.



.. class:: LatentVariableLayer(prior: tensorflow_probability.distributions.Distribution, encoder: tensorflow.keras.layers.Layer, compositor: Optional[tf.keras.layers.Layer] = None, name: Optional[str] = None)


   Bases: :class:`gpflux.layers.latent_variable_layer.LayerWithObservations`

   A latent variable layer, with amortized mean-field variational inference.

   The latent variable is distribution-agnostic, but assumes a variational posterior
   that is fully factorised and is of the same distribution family as the prior.

   This class is used by models as described in :cite:p:`dutordoir2018cde, salimbeni2019iwvi`.

   :param prior: A distribution that represents the :attr:`prior` over the latent variable.
   :param encoder: A layer which is passed the concatenated observation inputs
       and targets, and returns the appropriate parameters for the approximate
       posterior distribution; see :attr:`encoder`.
   :param compositor: A layer that combines layer inputs and latent variable
       samples into a single tensor; see :attr:`compositor`. If you do not specify a value for
       this parameter, the default is ``tf.keras.layers.Concatenate(axis=-1)``.
   :param name: The name of this layer (passed through to `tf.keras.layers.Layer`).

   .. attribute:: prior
      :annotation: :tfp.distributions.Distribution

      The prior distribution for the latent variables. 


   .. attribute:: encoder
      :annotation: :tf.keras.layers.Layer

      An encoder that maps from a concatenation of inputs and targets to the
      parameters of the approximate posterior distribution of the corresponding
      latent variables.


   .. attribute:: compositor
      :annotation: :tf.keras.layers.Layer

      A layer that takes as input the two-element ``[layer_inputs, latent_variable_samples]`` list
      and combines the elements into a single output tensor.


   .. method:: call(self, layer_inputs: gpflow.base.TensorType, observations: Optional[ObservationType] = None, training: Optional[bool] = None, seed: Optional[int] = None) -> tensorflow.Tensor

      Sample the latent variables and compose them with the layer input.

      When training, draw a sample of the latent variable from the posterior,
      whose distribution is parameterised by the encoder mapping from the data.
      Also add a KL divergence [posterior∥prior] to the losses.

      When not training, draw a sample of the latent variable from the prior.

      :param layer_inputs: The output of the previous layer.
      :param observations: The ``[inputs, targets]``, with the shapes ``[batch size, Din]``
          and ``[batch size, Dout]`` respectively. This parameter should be passed only when in
          training mode.
      :param training: The training mode indicator.
      :param seed: A random seed for the sampling operation.
      :returns: Samples of the latent variable composed with the layer inputs through the
          :attr:`compositor`


   .. method:: _inference_posteriors(self, observations: gpflux.types.ObservationType, training: Optional[bool] = None) -> tensorflow_probability.distributions.Distribution

      Return the posterior distributions parametrised by the :attr:`encoder`, which gets called
      with the concatenation of the inputs and targets in the *observations* argument.

      .. todo:: We might want to change encoders to have a
          `tfp.layers.DistributionLambda` final layer that directly returns the
          appropriately parameterised distributions object.

      :param observations: The ``[inputs, targets]``, with the shapes ``[batch size, Din]``
          and ``[batch size, Dout]`` respectively.
      :param training: The training mode indicator (passed through to the :attr:`encoder`'s call).
      :returns: The posterior distributions object.


   .. method:: _inference_latent_samples_and_loss(self, layer_inputs: gpflow.base.TensorType, observations: gpflux.types.ObservationType, seed: Optional[int] = None) -> Tuple[(tf.Tensor, tf.Tensor)]

      Sample latent variables during the *training* forward pass, hence requiring
      the observations. Also return the KL loss per datapoint.

      :param layer_inputs: The output of the previous layer _(unused)_.
      :param observations: The ``[inputs, targets]``, with the shapes ``[batch size, Din]``
          and ``[batch size, Dout]`` respectively.
      :param seed: A random seed for the sampling operation.
      :returns: The samples and the loss-per-datapoint.


   .. method:: _prediction_latent_samples(self, layer_inputs: gpflow.base.TensorType, seed: Optional[int] = None) -> tensorflow.Tensor

      Sample latent variables during the *prediction* forward pass, only
      depending on the shape of this layer's inputs.

      :param layer_inputs: The output of the previous layer (for determining batch shape).
      :param seed: A random seed for the sampling operation.
      :returns: The samples.


   .. method:: _local_kls(self, posteriors: tensorflow_probability.distributions.Distribution) -> tensorflow.Tensor

      Compute the KL divergences [posteriors∥prior].

      :param posteriors: A distribution that represents the approximate posteriors.
      :returns: The KL divergences from the prior for each of the posteriors.



.. class:: LayerWithObservations(trainable=True, name=None, dtype=None, dynamic=False, **kwargs)


   Bases: :class:`gpflux.layers.trackable_layer.TrackableLayer`

   By inheriting from this class, Layers indicate that their :meth:`call`
   method takes a second *observations* argument after the customary
   *layer_inputs* argument.

   This is used to distinguish which layers (unlike most standard Keras
   layers) require the original inputs and/or targets during training.
   For example, it is used by the amortized variational inference in the
   :class:`LatentVariableLayer`.

   .. method:: call(self, layer_inputs: gpflow.base.TensorType, observations: Optional[ObservationType] = None, training: Optional[bool] = None) -> tensorflow.Tensor
      :abstractmethod:

      The :meth:`call` method of `LayerWithObservations` subclasses should
      accept a second argument, *observations*. In training mode, this will
      be the ``[inputs, targets]`` of the training points; otherwise, it is `None`.



.. class:: LikelihoodLayer(likelihood: gpflow.likelihoods.Likelihood)


   Bases: :class:`gpflux.layers.trackable_layer.TrackableLayer`

   A Keras layer that wraps a GPflow :class:`~gpflow.likelihoods.Likelihood`. This layer expects a
   `tfp.distributions.MultivariateNormalDiag` as its input, describing ``q(f)``.
   When training, calling this class computes the negative variational expectation
   :math:`-\mathbb{E}_{q(f)}[\log p(y|f)]` and adds it as a layer loss.
   When not training, it computes the mean and variance of ``y`` under ``q(f)``
   using :meth:`~gpflow.likelihoods.Likelihood.predict_mean_and_var`.

   .. note::

       Use **either** this `LikelihoodLayer` (together with
       `gpflux.models.DeepGP`) **or** `LikelihoodLoss` (e.g. together with a
       `tf.keras.Sequential` model). Do **not** use both at once because
       this would add the loss twice.

   .. method:: call(self, inputs: tensorflow_probability.distributions.MultivariateNormalDiag, targets: Optional[TensorType] = None, training: bool = None) -> gpflux.layers.likelihood_layer.LikelihoodOutputs

      When training (``training=True``), this method computes variational expectations
      (data-fit loss) and adds this information as a layer loss.
      When testing (the default), it computes the posterior mean and variance of ``y``.

      :param inputs: The output distribution of the previous layer. This is currently
          expected to be a :class:`~tfp.distributions.MultivariateNormalDiag`;
          that is, the preceding :class:`~gpflux.layers.GPLayer` should have
          ``full_cov=full_output_cov=False``.
      :returns: a `LikelihoodOutputs` tuple with the mean and variance of ``f`` and,
          if not training, the mean and variance of ``y``.

      .. todo:: Turn this layer into a
          :class:`~tfp.layers.DistributionLambda` as well and return the
          correct :class:`~tfp.distributions.Distribution` instead of a tuple
          containing mean and variance only.



.. class:: TrackableLayer(trainable=True, name=None, dtype=None, dynamic=False, **kwargs)


   Bases: :class:`tensorflow.keras.layers.Layer`

   A :class:`tf.Layer` that tracks variables in :class:`tf.Module`\ s.

   .. todo:: Once TensorFlow 2.5 is released, this class will be removed.
       See https://github.com/Prowler-io/gpflux/issues/189

   .. method:: _submodules(self) -> Sequence[tf.Module]
      :property:

      Returns list of :class:`tf.Module` instances that are attributes on the class.
      This also includes instances within lists or tuples.


   .. method:: submodule_variables(self) -> Sequence[tf.Variable]

      Return flat iterable of variables from all attributes that contain `tf.Module`\ s


   .. method:: submodule_trainable_variables(self) -> Sequence[tf.Variable]

      Return flat iterable of trainable variables from all attributes that contain `tf.Module`\ s


   .. method:: submodule_non_trainable_variables(self) -> Sequence[tf.Variable]

      Return flat iterable of non-trainable variables from all
      attributes that contain `tf.Module`\ s


   .. method:: _dedup_weights(self, weights)

      Deduplicate weights while maintaining order as much as possible.


   .. method:: trainable_weights(self) -> Sequence[tf.Variable]
      :property:

      List of all trainable weights tracked by this layer.

      Unlike `tf.keras.layers.Layer`, this *will* track the weights of
      nested `tf.Module`\ s that are not themselves Keras layers.


   .. method:: non_trainable_weights(self) -> Sequence[tf.Variable]
      :property:

      List of all non-trainable weights tracked by this layer.

      Unlike `tf.keras.layers.Layer`, this *will* track the weights of
      nested `tf.Module`\ s that are not themselves Keras layers.


   .. method:: trainable_variables(self) -> Sequence[tf.Variable]
      :property:

      Sequence of trainable variables owned by this module and its submodules.

      Unlike `tf.keras.layers.Layer`, this *will* track the weights of
      nested `tf.Module`\ s that are not themselves Keras layers.


   .. method:: variables(self) -> Sequence[tf.Variable]
      :property:

      Returns the list of all layer variables/weights.

      Unlike `tf.keras.layers.Layer`, this *will* track the weights of
      nested `tf.Module`\ s that are not themselves Keras layers.



